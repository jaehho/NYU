{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dceecada",
   "metadata": {},
   "source": [
    "# Unsupervised Birdsong Syllable Pipeline — Notebook Skeleton\n",
    "_Created: 2025-11-07T00:58:12_\n",
    "## Overview\n",
    "This notebook scaffolds an end‑to‑end, label‑free pipeline:\n",
    "1. Load audio → spectrogram\n",
    "2. Event detection (connected components)\n",
    "3. Split–merge clustering to form templates\n",
    "4. Template merging (normalized L2 threshold)\n",
    "5. Greedy matching pursuit over time × frequency with non‑overlap collar\n",
    "6. Evaluation and simple visualizations\n",
    "\n",
    "Fill in TODOs with species‑specific details and thresholds.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b857cf2",
   "metadata": {},
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e59aaf9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configured.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import scipy.signal as sps\n",
    "import scipy.ndimage as ndi\n",
    "\n",
    "# If you have librosa installed, uncomment\n",
    "import librosa\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Clustering (install hdbscan if available)\n",
    "try:\n",
    "    import hdbscan\n",
    "except Exception as e:\n",
    "    hdbscan = None\n",
    "    print(\"HDBSCAN not available. Install with `pip install hdbscan`.\")\n",
    "\n",
    "# Random seed for reproducibility\n",
    "np.random.seed(0)\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = Path(\"../data/raw/\")                # TODO: set to your audio folder\n",
    "WORK_DIR = Path(\"../data/interim/\")               # artifacts go here\n",
    "WORK_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Audio parameters\n",
    "SR = 30000                               # sample rate Hz (TODO: set for your data)\n",
    "N_FFT = 1024\n",
    "HOP = 256\n",
    "FMIN = 300\n",
    "FMAX = 10000\n",
    "\n",
    "# Detection parameters\n",
    "THRESH_DB = 10.0                         # energy threshold in dB above noise floor\n",
    "MIN_DUR_S = 0.015                        # min syllable duration\n",
    "MAX_DUR_S = 0.5                          # max syllable duration\n",
    "MIN_GAP_S = 0.005                        # collar/gap for non-overlap\n",
    "\n",
    "# Clustering parameters\n",
    "MIN_CLUSTER_SIZE = 10\n",
    "MAX_CLUSTER_SIZE = 200\n",
    "MERGE_L2_CUTOFF = 0.33                   # normalized L2 threshold for template merge\n",
    "\n",
    "# Matching pursuit parameters\n",
    "MAX_ITERS = 1                            # number of global passes\n",
    "COLLAR_FRAMES = 2                        # non-overlap collar in frames\n",
    "\n",
    "print(\"Configured.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9df076f",
   "metadata": {},
   "source": [
    "## 1. Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88e7dcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_wav_mono(path: Path, sr: int) -> np.ndarray:\n",
    "    \"\"\"Load audio file as mono float32 at target sr. TODO: replace with librosa if available.\"\"\"\n",
    "    # Placeholder: generate silence for skeleton use\n",
    "    # Implement: y, _ = librosa.load(path.as_posix(), sr=sr, mono=True)\n",
    "    y = np.zeros(sr * 2, dtype=np.float32)  # 2 seconds of silence\n",
    "    return y\n",
    "\n",
    "def power_to_db(S: np.ndarray, ref: float = 1.0, amin: float = 1e-10) -> np.ndarray:\n",
    "    S = np.maximum(S, amin)\n",
    "    return 10.0 * np.log10(S / ref)\n",
    "\n",
    "def stft_spectrogram(y: np.ndarray, sr: int, n_fft: int, hop: int) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"Return magnitude spectrogram (power), freqs, times.\"\"\"\n",
    "    f, t, Zxx = sps.stft(y, fs=sr, nperseg=n_fft, noverlap=n_fft - hop, boundary=None, padded=False)\n",
    "    S = np.abs(Zxx) ** 2\n",
    "    return S, f, t\n",
    "\n",
    "def bandpass_mask(f: np.ndarray, fmin: float, fmax: float) -> np.ndarray:\n",
    "    return (f >= fmin) & (f <= fmax)\n",
    "\n",
    "def normalize_template(T: np.ndarray) -> np.ndarray:\n",
    "    norm = np.linalg.norm(T.ravel()) + 1e-12\n",
    "    return T / norm\n",
    "\n",
    "def normalized_l2(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    a_n = normalize_template(a)\n",
    "    b_n = normalize_template(b)\n",
    "    return np.linalg.norm(a_n - b_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e140ac",
   "metadata": {},
   "source": [
    "## 2. Event detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "755fb6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Event:\n",
    "    t0: int\n",
    "    t1: int\n",
    "    f0: int\n",
    "    f1: int\n",
    "\n",
    "def detect_events(S_db: np.ndarray, thresh_db: float, min_dur_frames: int) -> List[Event]:\n",
    "    \"\"\"Connected components in time-frequency above threshold.\"\"\"\n",
    "    mask = S_db > thresh_db\n",
    "    labeled, nlab = ndi.label(mask)\n",
    "    events: List[Event] = []\n",
    "    for lab in range(1, nlab + 1):\n",
    "        ys, xs = np.where(labeled == lab)\n",
    "        if xs.size == 0 or ys.size == 0:\n",
    "            continue\n",
    "        t0, t1 = xs.min(), xs.max() + 1\n",
    "        f0, f1 = ys.min(), ys.max() + 1\n",
    "        if (t1 - t0) >= min_dur_frames:\n",
    "            events.append(Event(t0, t1, f0, f1))\n",
    "    return events\n",
    "\n",
    "# TODO: refine threshold using noise floor estimate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f3133e",
   "metadata": {},
   "source": [
    "## 3. Feature extraction for clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e8359fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_event_patch(S: np.ndarray, ev: Event, pad_t: int = 4, pad_f: int = 4) -> np.ndarray:\n",
    "    t0 = max(0, ev.t0 - pad_t); t1 = min(S.shape[1], ev.t1 + pad_t)\n",
    "    f0 = max(0, ev.f0 - pad_f); f1 = min(S.shape[0], ev.f1 + pad_f)\n",
    "    patch = S[f0:f1, t0:t1]\n",
    "    return patch\n",
    "\n",
    "def resize_to_box(patch: np.ndarray, tgt_shape: Tuple[int, int] = (64, 64)) -> np.ndarray:\n",
    "    \"\"\"Resize using simple zoom to fixed box. TODO: replace with better interpolation.\"\"\"\n",
    "    zoom_f = (tgt_shape[0] / patch.shape[0], tgt_shape[1] / patch.shape[1])\n",
    "    return ndi.zoom(patch, zoom_f, order=1)\n",
    "\n",
    "def patches_to_features(patches: List[np.ndarray]) -> np.ndarray:\n",
    "    \"\"\"Global PCA placeholder: flatten vectors. TODO: add PCA if needed.\"\"\"\n",
    "    flat = [p.astype(np.float32).ravel() for p in patches]\n",
    "    X = np.vstack(flat) if flat else np.zeros((0, 4096), dtype=np.float32)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b257972",
   "metadata": {},
   "source": [
    "## 4. Split–merge clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc89a890",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hdbscan_cluster(X: np.ndarray, min_size: int, max_size: int) -> np.ndarray:\n",
    "    if hdbscan is None or X.shape[0] == 0:\n",
    "        return np.array([], dtype=int)\n",
    "    clusterer = hdbscan.HDBSCAN(min_cluster_size=min_size, min_samples=None)\n",
    "    labels = clusterer.fit_predict(X)\n",
    "    return labels\n",
    "\n",
    "def make_templates(patches: List[np.ndarray], labels: np.ndarray) -> Dict[int, np.ndarray]:\n",
    "    templates: Dict[int, np.ndarray] = {}\n",
    "    for lab in np.unique(labels):\n",
    "        if lab < 0:\n",
    "            continue\n",
    "        group = [p for p, l in zip(patches, labels) if l == lab]\n",
    "        if not group:\n",
    "            continue\n",
    "        # Median image as template\n",
    "        stack = np.stack([resize_to_box(p) for p in group], axis=0)\n",
    "        T = np.median(stack, axis=0)\n",
    "        templates[int(lab)] = normalize_template(T)\n",
    "    return templates\n",
    "\n",
    "def merge_templates(templates: Dict[int, np.ndarray], cutoff: float) -> Dict[int, np.ndarray]:\n",
    "    \"\"\"Agglomerative merge by normalized L2 distance with simple greedy pass.\"\"\"\n",
    "    labs = list(templates.keys())\n",
    "    keep = {lab: True for lab in labs}\n",
    "    rep = {lab: lab for lab in labs}\n",
    "    for i, li in enumerate(labs):\n",
    "        if not keep[li]: \n",
    "            continue\n",
    "        for lj in labs[i+1:]:\n",
    "            if not keep.get(lj, False):\n",
    "                continue\n",
    "            d = normalized_l2(templates[li], templates[lj])\n",
    "            if d < cutoff:\n",
    "                # merge lj into li\n",
    "                keep[lj] = False\n",
    "                rep[lj] = li\n",
    "    merged = {}\n",
    "    for lj, ok in keep.items():\n",
    "        if ok:\n",
    "            merged[lj] = templates[lj]\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed074da",
   "metadata": {},
   "source": [
    "## 5. Greedy matching pursuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e1e5c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Placement:\n",
    "    t: int\n",
    "    f: int\n",
    "    lab: int\n",
    "    score: float\n",
    "\n",
    "def match_score(S: np.ndarray, T: np.ndarray, t: int, f: int) -> float:\n",
    "    H, W = T.shape\n",
    "    if f + H > S.shape[0] or t + W > S.shape[1]:\n",
    "        return -np.inf\n",
    "    window = S[f:f+H, t:t+W]\n",
    "    # cosine similarity proxy via normalized L2\n",
    "    denom = (np.linalg.norm(window.ravel()) * np.linalg.norm(T.ravel())) + 1e-10\n",
    "    return float(np.dot(window.ravel(), T.ravel()) / denom)\n",
    "\n",
    "def greedy_decompose(S: np.ndarray, templates: Dict[int, np.ndarray], collar_frames: int = 2, max_iters: int = 1) -> List[Placement]:\n",
    "    placed: List[Placement] = []\n",
    "    busy = np.zeros(S.shape[1], dtype=bool)  # time busy mask\n",
    "    for _ in range(max_iters):\n",
    "        improved = False\n",
    "        for lab, T in templates.items():\n",
    "            H, W = T.shape\n",
    "            for t in range(0, S.shape[1] - W):\n",
    "                if busy[max(0, t-collar_frames):min(S.shape[1], t+W+collar_frames)].any():\n",
    "                    continue\n",
    "                # naive frequency search\n",
    "                for f in range(0, S.shape[0] - H):\n",
    "                    sc = match_score(S, T, t, f)\n",
    "                    if sc > 0.6:  # TODO: tune threshold\n",
    "                        placed.append(Placement(t=t, f=f, lab=lab, score=sc))\n",
    "                        busy[max(0, t):min(S.shape[1], t+W)] = True\n",
    "                        improved = True\n",
    "                        break\n",
    "        if not improved:\n",
    "            break\n",
    "    return placed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73adbab8",
   "metadata": {},
   "source": [
    "## 6. Evaluation scaffolding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b13cc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_syllables(placements: List[Placement]) -> Dict[int, int]:\n",
    "    counts: Dict[int, int] = {}\n",
    "    for p in placements:\n",
    "        counts[p.lab] = counts.get(p.lab, 0) + 1\n",
    "    return counts\n",
    "\n",
    "def simple_precision(tp: int, fp: int) -> float:\n",
    "    return tp / max(1, (tp + fp))\n",
    "\n",
    "# TODO: add matching to any available ground truth if present.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b6ecb2",
   "metadata": {},
   "source": [
    "## 7. Visualization helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c444eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_spectrogram(S_db: np.ndarray, f: np.ndarray, t: np.ndarray, title: str = \"Spectrogram (dB)\") -> None:\n",
    "    plt.figure()\n",
    "    plt.imshow(S_db, aspect='auto', origin='lower', extent=[t.min(), t.max(), f.min(), f.max()])\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.ylabel(\"Frequency (Hz)\")\n",
    "    plt.title(title)\n",
    "    plt.colorbar(label=\"dB\")\n",
    "    plt.show()\n",
    "\n",
    "def overlay_placements(S_db: np.ndarray, f: np.ndarray, t: np.ndarray, placements: List[Placement], templates: Dict[int, np.ndarray]) -> None:\n",
    "    plt.figure()\n",
    "    plt.imshow(S_db, aspect='auto', origin='lower', extent=[t.min(), t.max(), f.min(), f.max()])\n",
    "    for p in placements:\n",
    "        T = templates[p.lab]\n",
    "        H, W = T.shape\n",
    "        t0 = t[p.t]; t1 = t[min(p.t + W, len(t)-1)]\n",
    "        f0 = f[p.f]; f1 = f[min(p.f + H, len(f)-1)]\n",
    "        plt.gca().add_patch(plt.Rectangle((t0, f0), (t1 - t0), (f1 - f0), fill=False, linewidth=1))\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.ylabel(\"Frequency (Hz)\")\n",
    "    plt.title(\"Placements overlay\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f19bdd0",
   "metadata": {},
   "source": [
    "## 8. End‑to‑end runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f8db94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jaeho/NYU/budgie/.venv/lib/python3.12/site-packages/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/jaeho/NYU/budgie/.venv/lib/python3.12/site-packages/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "k must be less than or equal to the number of training points",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 38\u001b[39m\n\u001b[32m     24\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m     25\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mevents\u001b[39m\u001b[33m\"\u001b[39m: events,\n\u001b[32m     26\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mpatches\u001b[39m\u001b[33m\"\u001b[39m: patches,\n\u001b[32m   (...)\u001b[39m\u001b[32m     34\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtimes\u001b[39m\u001b[33m\"\u001b[39m: times,\n\u001b[32m     35\u001b[39m     }\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# Example usage (disabled by default)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m result = \u001b[43mrun_on_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mWORK_DIR\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minteresting.flac\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# TODO: set path\u001b[39;00m\n\u001b[32m     39\u001b[39m show_spectrogram(result[\u001b[33m\"\u001b[39m\u001b[33mS_db\u001b[39m\u001b[33m\"\u001b[39m], result[\u001b[33m\"\u001b[39m\u001b[33mfreqs\u001b[39m\u001b[33m\"\u001b[39m], result[\u001b[33m\"\u001b[39m\u001b[33mtimes\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mrun_on_file\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m     15\u001b[39m X = patches_to_features(patches)\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# 5) Cluster and template\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m labels = \u001b[43mhdbscan_cluster\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMIN_CLUSTER_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMAX_CLUSTER_SIZE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m templates = make_templates(patches, labels) \u001b[38;5;28;01mif\u001b[39;00m labels.size \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[32m     19\u001b[39m templates = merge_templates(templates, MERGE_L2_CUTOFF) \u001b[38;5;28;01mif\u001b[39;00m templates \u001b[38;5;28;01melse\u001b[39;00m {}\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mhdbscan_cluster\u001b[39m\u001b[34m(X, min_size, max_size)\u001b[39m\n\u001b[32m      3\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m np.array([], dtype=\u001b[38;5;28mint\u001b[39m)\n\u001b[32m      4\u001b[39m clusterer = hdbscan.HDBSCAN(min_cluster_size=min_size, min_samples=\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m labels = \u001b[43mclusterer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m labels\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/NYU/budgie/.venv/lib/python3.12/site-packages/hdbscan/hdbscan_.py:1312\u001b[39m, in \u001b[36mHDBSCAN.fit_predict\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m   1297\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit_predict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m   1298\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Performs clustering on X and returns cluster labels.\u001b[39;00m\n\u001b[32m   1299\u001b[39m \n\u001b[32m   1300\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1310\u001b[39m \u001b[33;03m        cluster labels\u001b[39;00m\n\u001b[32m   1311\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1312\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1313\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.labels_\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/NYU/budgie/.venv/lib/python3.12/site-packages/hdbscan/hdbscan_.py:1272\u001b[39m, in \u001b[36mHDBSCAN.fit\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m   1262\u001b[39m kwargs.update(\u001b[38;5;28mself\u001b[39m._metric_kwargs)\n\u001b[32m   1263\u001b[39m kwargs[\u001b[33m'\u001b[39m\u001b[33mgen_min_span_tree\u001b[39m\u001b[33m'\u001b[39m] |= \u001b[38;5;28mself\u001b[39m.branch_detection_data\n\u001b[32m   1265\u001b[39m (\n\u001b[32m   1266\u001b[39m     \u001b[38;5;28mself\u001b[39m.labels_,\n\u001b[32m   1267\u001b[39m     \u001b[38;5;28mself\u001b[39m.probabilities_,\n\u001b[32m   1268\u001b[39m     \u001b[38;5;28mself\u001b[39m.cluster_persistence_,\n\u001b[32m   1269\u001b[39m     \u001b[38;5;28mself\u001b[39m._condensed_tree,\n\u001b[32m   1270\u001b[39m     \u001b[38;5;28mself\u001b[39m._single_linkage_tree,\n\u001b[32m   1271\u001b[39m     \u001b[38;5;28mself\u001b[39m._min_spanning_tree,\n\u001b[32m-> \u001b[39m\u001b[32m1272\u001b[39m ) = \u001b[43mhdbscan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclean_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1274\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.metric != \u001b[33m\"\u001b[39m\u001b[33mprecomputed\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._all_finite:\n\u001b[32m   1275\u001b[39m     \u001b[38;5;66;03m# remap indices to align with original data in the case of non-finite entries.\u001b[39;00m\n\u001b[32m   1276\u001b[39m     \u001b[38;5;28mself\u001b[39m._condensed_tree = remap_condensed_tree(\n\u001b[32m   1277\u001b[39m         \u001b[38;5;28mself\u001b[39m._condensed_tree, internal_to_raw, outliers\n\u001b[32m   1278\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/NYU/budgie/.venv/lib/python3.12/site-packages/hdbscan/hdbscan_.py:855\u001b[39m, in \u001b[36mhdbscan\u001b[39m\u001b[34m(X, min_cluster_size, min_samples, alpha, cluster_selection_epsilon, cluster_selection_persistence, max_cluster_size, metric, p, leaf_size, algorithm, memory, approx_min_span_tree, gen_min_span_tree, core_dist_n_jobs, cluster_selection_method, allow_single_cluster, match_reference_implementation, cluster_selection_epsilon_max, **kwargs)\u001b[39m\n\u001b[32m    851\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m metric \u001b[38;5;129;01min\u001b[39;00m KDTREE_VALID_METRICS:\n\u001b[32m    852\u001b[39m     \u001b[38;5;66;03m# TO DO: Need heuristic to decide when to go to boruvka;\u001b[39;00m\n\u001b[32m    853\u001b[39m     \u001b[38;5;66;03m# still debugging for now\u001b[39;00m\n\u001b[32m    854\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m X.shape[\u001b[32m1\u001b[39m] > \u001b[32m60\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m855\u001b[39m         (single_linkage_tree, result_min_span_tree) = \u001b[43mmemory\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    856\u001b[39m \u001b[43m            \u001b[49m\u001b[43m_hdbscan_prims_kdtree\u001b[49m\n\u001b[32m    857\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    858\u001b[39m \u001b[43m            \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    859\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmin_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    860\u001b[39m \u001b[43m            \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    861\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    862\u001b[39m \u001b[43m            \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    863\u001b[39m \u001b[43m            \u001b[49m\u001b[43mleaf_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    864\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgen_min_span_tree\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    865\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    866\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    867\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    868\u001b[39m         (single_linkage_tree, result_min_span_tree) = memory.cache(\n\u001b[32m    869\u001b[39m             _hdbscan_boruvka_kdtree\n\u001b[32m    870\u001b[39m         )(\n\u001b[32m   (...)\u001b[39m\u001b[32m    880\u001b[39m             **kwargs\n\u001b[32m    881\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/NYU/budgie/.venv/lib/python3.12/site-packages/joblib/memory.py:326\u001b[39m, in \u001b[36mNotMemorizedFunc.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/NYU/budgie/.venv/lib/python3.12/site-packages/hdbscan/hdbscan_.py:266\u001b[39m, in \u001b[36m_hdbscan_prims_kdtree\u001b[39m\u001b[34m(X, min_samples, alpha, metric, p, leaf_size, gen_min_span_tree, **kwargs)\u001b[39m\n\u001b[32m    263\u001b[39m dist_metric = DistanceMetric.get_metric(metric, **kwargs)\n\u001b[32m    265\u001b[39m \u001b[38;5;66;03m# Get distance to kth nearest neighbour\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m266\u001b[39m core_distances = \u001b[43mtree\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmin_samples\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdualtree\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbreadth_first\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m    268\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m][:, -\u001b[32m1\u001b[39m].copy(order=\u001b[33m\"\u001b[39m\u001b[33mC\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    270\u001b[39m \u001b[38;5;66;03m# Mutual reachability distance is implicit in mst_linkage_core_vector\u001b[39;00m\n\u001b[32m    271\u001b[39m min_spanning_tree = mst_linkage_core_vector(X, core_distances, dist_metric, alpha)\n",
      "\u001b[36mFile \u001b[39m\u001b[32msklearn/neighbors/_binary_tree.pxi:1180\u001b[39m, in \u001b[36msklearn.neighbors._kd_tree.BinaryTree64.query\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mValueError\u001b[39m: k must be less than or equal to the number of training points"
     ]
    }
   ],
   "source": [
    "def run_on_file(path: Path) -> Dict[str, Any]:\n",
    "    # 1) Load\n",
    "    y = read_wav_mono(path, SR)\n",
    "    # 2) Spectrogram\n",
    "    S, freqs, times = stft_spectrogram(y, SR, N_FFT, HOP)\n",
    "    bp = bandpass_mask(freqs, FMIN, FMAX)\n",
    "    S = S[bp, :]\n",
    "    S_db = power_to_db(S, ref=np.max(S)+1e-12)\n",
    "    # 3) Detect events\n",
    "    min_dur_frames = int(MIN_DUR_S * SR / HOP)\n",
    "    events = detect_events(S_db, THRESH_DB, min_dur_frames)\n",
    "    # 4) Patches and features\n",
    "    patches = [extract_event_patch(S_db, ev) for ev in events]\n",
    "    patches = [resize_to_box(p) for p in patches if p.size > 0]\n",
    "    X = patches_to_features(patches)\n",
    "    # 5) Cluster and template\n",
    "    labels = hdbscan_cluster(X, MIN_CLUSTER_SIZE, MAX_CLUSTER_SIZE)\n",
    "    templates = make_templates(patches, labels) if labels.size else {}\n",
    "    templates = merge_templates(templates, MERGE_L2_CUTOFF) if templates else {}\n",
    "    # 6) Matching pursuit\n",
    "    placements = greedy_decompose(S_db, templates, COLLAR_FRAMES, MAX_ITERS) if templates else []\n",
    "    # 7) Outputs\n",
    "    bos = bag_of_syllables(placements)\n",
    "    return {\n",
    "        \"events\": events,\n",
    "        \"patches\": patches,\n",
    "        \"X_shape\": X.shape,\n",
    "        \"labels\": labels,\n",
    "        \"templates\": templates,\n",
    "        \"placements\": placements,\n",
    "        \"bag_of_syllables\": bos,\n",
    "        \"S_db\": S_db,\n",
    "        \"freqs\": freqs[bp],\n",
    "        \"times\": times,\n",
    "    }\n",
    "\n",
    "# Example usage (disabled by default)\n",
    "result = run_on_file(WORK_DIR / \"interesting.flac\")  # TODO: set path\n",
    "show_spectrogram(result[\"S_db\"], result[\"freqs\"], result[\"times\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ed6f93",
   "metadata": {},
   "source": [
    "## 9. TODOs\n",
    "- Replace the dummy audio loader with librosa or soundfile.\n",
    "- Add PCA before HDBSCAN and a second per‑cluster PCA+HDBSCAN split pass.\n",
    "- Improve thresholding with local noise floor estimation.\n",
    "- Tune template merge cutoff and matching threshold per species.\n",
    "- Add evaluation against any available annotations.\n",
    "- Persist artifacts: templates, placements, and embeddings.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "budgie",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
